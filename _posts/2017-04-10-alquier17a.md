---
title: Regret Bounds for Lifelong Learning
abstract: We consider the problem of transfer learning in an online setting. Different
  tasks are presented sequentially  and processed by a within-task algorithm. We propose
  a lifelong learning strategy which refines the underlying  data representation used
  by the within-task algorithm, thereby transferring information from one task to
  the next.  We show that when the within-task algorithm comes with some regret bound,
  our strategy inherits this good property.  Our bounds are in expectation for a general
  loss function, and uniform for a convex loss. We discuss applications to dictionary
  learning and finite set of predictors. In the latter case, we improve previous $O(1/\sqrt{m})$
  bounds  to $O(1/m)$, where $m$ is the per task sample size.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: alquier17a
month: 0
tex_title: "{Regret Bounds for Lifelong Learning}"
firstpage: 261
lastpage: 269
page: 261-269
order: 261
cycles: false
author:
- given: Pierre
  family: Alquier
- given: The Tien
  family: Mai
- given: Massimiliano
  family: Pontil
date: 2017-04-10
address:
publisher: PMLR
container-title: Proceedings of the 20th International Conference on Artificial Intelligence
  and Statistics
volume: '54'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 4
  - 10
pdf: http://proceedings.mlr.press/v54/alquier17a/alquier17a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v54/alquier17a/alquier17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
